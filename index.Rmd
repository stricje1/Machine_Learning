---
title: "Machine Learning and Exercise Gadgets"
author: "Jeffrey Strickland"
date: "1/14/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, fig.width=7, fig.height=5, scipen = 1000000)
```

```{r echo=FALSE}
library(mlbench)
library(caret)
library(randomForest)
library(rpart) 
library(rattle)
library(cvms)
library(rsvg)
library(ggimage)
library(rpart.plot)
library(RColorBrewer)
set.seed(1234)
```

## Data Preprocessing

The training an d test data for this project are available here:

```{r}
trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
```

### Data Preparation
Now we read the data into memory and remove fissing data

```{r}
training <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))
```

Here, we construct the training and testing dataframes to prepare the data for cross-validation.
```{r}
inTrain <- createDataPartition(y=training$classe, p=0.6, list=FALSE)
myTraining <- training[inTrain, ]; myTesting <- training[-inTrain, ]
dim(myTraining); dim(myTesting)
```

### Cleaning the data

The following transformations were used to clean the data:
  
Transformation 1: Cleaning NearZeroVariance Variables Run this code to view possible NZV Variables:
  
```{r }
myDataNZV <- nearZeroVar(myTraining, saveMetrics=TRUE)
```

```{r echo=FALSE}
myNZVvars <- names(myTraining) %in% c("new_window", "kurtosis_roll_belt", "kurtosis_picth_belt",
"kurtosis_yaw_belt", "skewness_roll_belt", "skewness_roll_belt.1", "skewness_yaw_belt",
"max_yaw_belt", "min_yaw_belt", "amplitude_yaw_belt", "avg_roll_arm", "stddev_roll_arm",
"var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm", "var_pitch_arm", "avg_yaw_arm",
"stddev_yaw_arm", "var_yaw_arm", "kurtosis_roll_arm", "kurtosis_picth_arm",
"kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm",
"max_roll_arm", "min_roll_arm", "min_pitch_arm", "amplitude_roll_arm", "amplitude_pitch_arm",
"kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell", "skewness_roll_dumbbell",
"skewness_pitch_dumbbell", "skewness_yaw_dumbbell", "max_yaw_dumbbell", "min_yaw_dumbbell",
"amplitude_yaw_dumbbell", "kurtosis_roll_forearm", "kurtosis_picth_forearm", "kurtosis_yaw_forearm",
"skewness_roll_forearm", "skewness_pitch_forearm", "skewness_yaw_forearm", "max_roll_forearm",
"max_yaw_forearm", "min_roll_forearm", "min_yaw_forearm", "amplitude_roll_forearm",
"amplitude_yaw_forearm", "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm",
"avg_pitch_forearm", "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm",
"stddev_yaw_forearm", "var_yaw_forearm")
```

```{r}
myTraining <- myTraining[!myNZVvars]
```

Transformation 2: Killing first column of Dataset - ID Removing first ID variable so that it does not interfere with ML Algorithms:
  
```{r}
myTraining <- myTraining[c(-1)]
```

Transformation 3: Cleaning Variables with too many NAs. For Variables that have more than a 60% threshold of NAs I’m going to leave them out. We also establish the training set to have 60% of the data, leaving 40% for the testing set.

```{r}  
trainingV3 <- myTraining #creating another subset to iterate in loop
for(i in 1:length(myTraining)) { #for every column in the training dataset
  if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .6 ) { #if n?? NAs > 60% of total observations
    for(j in 1:length(trainingV3)) {
      if( length( grep(names(myTraining[i]), names(trainingV3)[j]) ) ==1)  { #if the columns are the same:
        trainingV3 <- trainingV3[ , -j] #Remove that column
      }   
    } 
  }
}
```

### Resetting the Training Set:
Next, we put the data into its orginal training set and remove (rm) the last iteration or it (trainingV3).

```{r}
myTraining <- trainingV3
rm(trainingV3)
```

Now we perform the exact same 3 transformations for myTesting and testing data sets.

```{r}
clean1 <- colnames(myTraining)
clean2 <- colnames(myTraining[, -58]) #already with classe column removed
myTesting <- myTesting[clean1]
testing <- testing[clean2]
```

### Data Coersion

In order to ensure proper functioning of the classifier algorithms with the test data set, we need to coerce the data into the same type. We also make sure Coersion works by row-binding row two of myTraining and then removing the row.

```{r}
for (i in 1:length(testing) ) {
  for(j in 1:length(myTraining)) {
    if( length( grep(names(myTraining[i]), names(testing)[j]) ) ==1)  {
      class(testing[j]) <- class(myTraining[i])
    }      
  }      
}
testing <- rbind(myTraining[2, -58] , testing) 
testing <- testing[-1,]
```
## Finding the Best ML Model

### Ensemble Learning 
Ensemble learning is a machine learning concept in which idea is to train multiple models (learners) to solve the same problem. The main advantages of Ensemble learning methods are: (1) reduced variance, which helps overcome overfitting helps the model to be independent of training data; and (2) reduced bias, which overcome underfitting problem improves reliable classification over a single classifier. 

#### *Random Forest*
The random forest algorithm is comprised of multiole trees, either classification or regression trees. This make it an ensemble model.

#### *Stochastic Gradient Descent (SGD)*
SGD is a simple, very efficient approach to fitting linear classifiers and regressors under convex loss functions such as (linear) SVMs and Logistic Regression. SGDs are efficient easy to omplement, as there are many opportunities for sode tuning.

### Bagging (Bootstrap Aggregating)
Bagging generates n new training data sets, each of which picks a sample of observations with replacement (bootstrap sample) from original data set.  By sampling with replacement, some observations may be repeated in each new training data set. The n models are fitted using the above n bootstrap samples and combined them by averaging the output (for regression) or voting (for classification).

#### *Boosted Logistic Regression*
In logistic regression, boosting works by sequentially applying a the algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers it produces

#### *Bagged Classification and Regression Trees (CART)* 
Bagging algorithms with recursive partitioning as the estimator result in a prediction tool that is no longer a tree. Thus, these methods have superior predictive accuracy when compared to a single tree. Recursive partitioning that incorporates cross-validation still results in a tree. Classification trees involve a categorical response variable and regression trees a continuous response variable. The approaches are similar enough to be referred to together as CART (Classification and Regression Trees). Predictors can be any combination of categorical or continuous variables. With bagging applied, mutiple trees are evaluated.

### Train ML Algorithms
if (!require("BiocManager", quietly = TRUE)) install.packages("BiocManager")
BiocManager::install("logicFS")
library(logicFS)

### Prepare Training Scheme
trainControl manages the computational nuances of the train function(). The method, repeatedcv, causes the parameters to be repeated, and 5-fold CV mean it divides the training set randomly into 5 parts and then using each of 5 parts as testing dataset for the model trained on other 4. With 3 repeats of 5 fold CV, the algorithms perform the average of 3 error terms obtained by performing 5 fold CV 3 times.

```{r}
control <- trainControl(method="repeatedcv", number=5, repeats=3)
# train the Logit Boost model
set.seed(7)
modelLog <- train(classe~., data=myTraining, method="LogitBoost", trControl=control)
# train the RF model
set.seed(7)
modelRf <- train(classe~., data=myTraining, method="rf", trControl=control)
# train the GBM model
set.seed(7)
modelGbm <- train(classe~., data=myTraining, method="gbm", trControl=control, verbose=FALSE)
# train the CART model
set.seed(7)
modelTb <- train(classe~., data=myTraining, method="treebag", trControl=control)
# collect resamples
results <- resamples(list(Log=modelLog, RF=modelRf, GBM=modelGbm, Tb=modelTb))
# summarize the distributions
summary(results)
```

The Accuracy metric shows that all the models fit the data well, and practically, these RF, GB, and TB perform equally. However, we must pick a “best” model, so we will take the RF, which performs slight better than the TB (by 0.06). It is interesting to see that both RF and TB are tree-ensembles.

### boxplots of results
The boxplot provides visual support of the above results.

```{r}
bwplot(results)
```

## Cross-Validation
Now, we use the testing data for cross-validation. This includes validating the model with the testing dataset. We also includes interpret the results and explain them using a graphical confusion matrix.

```{r}
preductionOnTesting <- predict(modelRf, newdata=testing)
preductionOnTesting
```

```{r}
Rf_pred <- predict(modelRf, myTesting)
conf_mat <- confusion_matrix(targets = myTesting$classe,
                             predictions = Rf_pred)
plot_confusion_matrix(conf_mat$`Confusion Matrix`[[1]])
```

In the middle of each tile, we have the normalized count (overall percentage) and, beneath it, the count. When all of the tile overall percentages are summed, they sum is 100%. For instance, 18.3% + 16.3% + 17.2% + 19.2% + 28.4% + 0.2% + 1% + 1% = 100%, allowing for round-off.

At the bottom of each tile, we have the column percentage. Of all the observations where Target is A, 99.8% of them were predicted to be A and 0.2% to be B. At the right side of each tile, we have the row percentage. Of all the observations where Prediction is B, 98.8% of them were actually B, while 1.2% were C. Note that the color intensity is based on the counts.

## References

Bioconductor (January 14, 2022). Using Bioconductor. https://bioconductor.org/install/

Brownlee, J. (September 24, 2014). "Compare Models And Select The Best Using The Caret R Package," R Machine Learning. https://machinelearningmastery.com/compare-models-and-select-the-best-using-the-caret-r-package/

CRAN R Project (October 9, 2021). Package ‘caret’. https://cran.r-project.org/web/packages/caret/caret.pdf

Kuhn, M. (March 27, 2019). train Models By Tag. http://topepo.github.io/caret/train-models-by-tag.html#logistic-regression

Olsen, L. R. (November 14, 2021). Creating a confusion matrix with cvms. https://cran.r-project.org/web/packages/cvms/vignettes/Creating_a_confusion_matrix.html

